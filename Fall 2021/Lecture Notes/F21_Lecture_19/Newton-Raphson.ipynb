{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "University of Michigan - ROB 101 Computational Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Newton-Raphson for Vector Functions\n",
    "\n",
    "- Let $x_k$ be our current approximation of a root of the function $f$. \n",
    "- We write the linear approximation of $f$ about $x_k$ as $$f(x) \\approx f(x_k) + A (x-x_k), \\quad A= \\frac{\\partial f(x_k)}{\\partial x}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Newton-Raphson for Vector Functions\n",
    "\n",
    "- Start with an initial guess $x_0$ ($k=0$).\n",
    "- Solve the linear system $A \\Delta x_k = -f(x_k)$.\n",
    "- Update the estimated root $x_{k+1} = x_k + \\Delta x_k$.\n",
    "- Repeat (go back to 2) until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Jacobian (generic function with 2 methods)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "function Jacobian(func, x0, h = 0.001) \n",
    "    # Numerical Jacobian of f:R^m -> R^n\n",
    "    m = length(x0);  # Domain dimension\n",
    "    f0 = func(x0); \n",
    "    n = length(f0); # Range dimension\n",
    "    \n",
    "    if m == 1 # f:R -> R^n\n",
    "        return (func(x0 .+ h) .- func(x0 .- h)) ./ (2*h)\n",
    "    else\n",
    "        Im = Matrix(1.0I, m, m); # Create standard basis for I_m\n",
    "        A = zeros(n,m); # Create Jacobian matrix\n",
    "        # Compute and fill in the columns of the Jacobian using central differences\n",
    "        for i = 1:m\n",
    "            ei = Im[:,i:i]\n",
    "            A[:,i] = (func(x0 + h*ei) - func(x0 - h*ei)) / (2*h);\n",
    "        end\n",
    "        return A\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 1\n",
    "\n",
    "Find a root of $F:\\mathbb{R}^4 \\to \\mathbb{R}^4$ near $x_0=\\left[\\begin{array}{cccc} -2.0 & 3.0 & \\pi &-1.0\\end{array} \\right]^\\top$ for\n",
    " \n",
    "$$\n",
    "F(x)=\\left[\\begin{array}{c}\n",
    "   x_1 + 2 x_2 - x_1 (x_1 + 4 x_2) - x_2 (4 x_1 + 10 x_2) + 3 \\\\\n",
    " 3 x_1 + 4 x_2 - x_1 (x_1 + 4 x_2) - x_2 (4 x_1 + 10 x_2) + 4  \\\\\n",
    "                                0.5 \\cos(x_1) + x_3 -\\left( \\sin(x_3) \\right)^7  \\\\\n",
    "                              -  2(x_2)^2  \\sin(x_1) + (x_4)^3\n",
    "\\end{array} \\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myfunc = f = f\n",
      "x0 = rand(4, 1) = [0.06279413918438403; 0.6901458522190622; 0.40080694340258494; 0.5896748599168935]\n",
      "f(x0): [-0.6705671512593088, 1.8353128315475833, 0.8984451993464828, 0.14526108696706044]\n",
      "A: [-4.646755096120669 -12.305270157856096 0.0 0.0; -2.6467550961208897 -10.305270157856095 0.0 0.0; -0.03137643477912899 0.0 0.9772639499319169 0.0; -0.9507249477548191 -0.17323455943413224 0.0 1.043150321254002]\n"
     ]
    }
   ],
   "source": [
    "function f(x) # f:R^4 -> R^4\n",
    "    f1 = x[1]+2*x[2]-x[1]*(x[1]+4*x[2])-x[2]*(4*x[1]+10*x[2])+3;\n",
    "    f2 = 3*x[1]+4*x[2]-x[1]*(x[1]+4*x[2])-x[2]*(4*x[1]+10*x[2])+4;\n",
    "    f3 = 0.5*cos(x[1])+x[3]-sin(x[3])^7;\n",
    "    f4 = -2*x[2]^2*sin(x[1])+x[4]^3;\n",
    "    return [f1; f2; f3; f4]\n",
    "end\n",
    "\n",
    "# Run this for testing\n",
    "@show myfunc = f;\n",
    "@show x0 = rand(4,1) # initial guess, x0\n",
    "println(\"f(x0): \", myfunc(x0))\n",
    "println(\"A: \", Jacobian(myfunc, x0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration: N = 7\n",
      "root: [-2.2595730873836657, 1.7595730873836666, 0.3180893310400119, -1.6845806986434577]\n",
      "f(root): [-5.329070518200751e-15, -3.552713678800501e-15, -4.358492733391728e-17, -3.531841485937548e-10]\n"
     ]
    }
   ],
   "source": [
    "myfunc = f;\n",
    "x0 = [-2.; 3.; π; -1.]; # initial guess\n",
    "delta = 1e-9; # set a convergence threshold\n",
    "# set a max iteration so we don't get stuck in the loop forever!\n",
    "MAX_ITER = 100; \n",
    "N = 1; # counter for iteration\n",
    "x = x0; # root variable\n",
    "# let the fun begin!\n",
    "while N < MAX_ITER\n",
    "    fk = myfunc(x); # evaluate f at current guess, x0\n",
    "    A = Jacobian(myfunc, x); # compute Jacobian\n",
    "    if abs(det(A)) < delta\n",
    "        println(\"Newton-Raphson method did not converge; Jacobian is singular.\")\n",
    "        break\n",
    "    else\n",
    "        dx = -A \\ fk; # solve the linear system\n",
    "    end\n",
    "    if norm(dx) < delta\n",
    "        println(\"Converged at iteration: N = \", N)\n",
    "        break\n",
    "    else\n",
    "        x += dx;\n",
    "    end\n",
    "    N += 1;\n",
    "end\n",
    "\n",
    "print(\"root: \", x, \"\\n\")\n",
    "print(\"f(root): \", myfunc(x), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [Automatic Differentiation (autodiff)](https://en.wikipedia.org/wiki/Automatic_differentiation)\n",
    "\n",
    "- Automatic differentiation is distinct from symbolic differentiation and numerical differentiation. \n",
    "- Symbolic differentiation faces the difficulty of converting a computer program into a single mathematical expression and can lead to inefficient code. \n",
    "- Numerical differentiation (the method of finite differences) can introduce round-off errors in the discretization process and cancellation. \n",
    "- Both of these classical methods have problems with calculating higher derivatives, where complexity and errors increase.\n",
    "- Finally, both of these classical methods are slow at computing partial derivatives of a function with respect to many inputs, as is needed for gradient-based optimization algorithms. \n",
    "- Automatic differentiation solves all of these problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m registry at `C:\\Users\\maanigj\\.julia\\registries\\General`\n",
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [>                                        ]  0.0 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [==>                                      ]  3.4 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [===>                                     ]  6.9 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [=====>                                   ]  10.3 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [======>                                  ]  13.8 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [=======>                                 ]  17.2 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [=========>                               ]  20.7 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [==========>                              ]  24.1 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [============>                            ]  27.6 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [=============>                           ]  31.0 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [==============>                          ]  34.5 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [================>                        ]  37.9 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [=================>                       ]  41.4 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [==================>                      ]  44.8 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [====================>                    ]  48.3 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [=====================>                   ]  51.7 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [=======================>                 ]  55.2 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [========================>                ]  58.6 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [=========================>               ]  62.1 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [===========================>             ]  65.5 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [============================>            ]  69.0 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [=============================>           ]  72.4 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [===============================>         ]  75.9 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [================================>        ]  79.3 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [==================================>      ]  82.8 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [===================================>     ]  86.2 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [====================================>    ]  89.7 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [======================================>  ]  93.1 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [=======================================> ]  96.6 %\r",
      "    \u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [========================================>]  100.0 %\r",
      "\u001b[2K\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m `C:\\Users\\maanigj\\.julia\\environments\\v1.4\\Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m `C:\\Users\\maanigj\\.julia\\environments\\v1.4\\Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.add(\"ForwardDiff\")\n",
    "\n",
    "using ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myfunc = f = f\n",
      "x0 = rand(4, 1) = [0.3620767464240069; 0.8948642769654549; 0.361817753236954; 0.48824943433977963]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float64,2}:\n",
       " -6.88307   -18.7939  0.0       0.0\n",
       " -4.88307   -16.7939  0.0       0.0\n",
       " -0.177109    0.0     0.987122  0.0\n",
       " -1.49772    -1.2679  0.0       0.715163"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this for testing\n",
    "@show myfunc = f;\n",
    "@show x0 = rand(4,1) # initial guess, x0\n",
    "\n",
    "ForwardDiff.jacobian(myfunc, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1.109165635397067e-6\n"
     ]
    }
   ],
   "source": [
    "# Compare with our finite difference Jacobian\n",
    "A_ad = ForwardDiff.jacobian(myfunc, x0);\n",
    "A_numerical = Jacobian(myfunc, x0)\n",
    "\n",
    "println(\"Error: \", norm(A_ad - A_numerical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![not-bad.jpg](https://i.postimg.cc/WzKBHQMM/not-bad.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Remark:** Automatic differentiation (diving inside the method) at this time is beyond ROB 101 scope. A good course to study after ROB 101 is [Introduction to Deep Learning, STAT 157, UC Berkeley, Spring, 2019.](https://c.d2l.ai/berkeley-stat-157/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Newton-Raphson using Automatic Differentiation\n",
    "\n",
    "We now implement our Newton-Raphson algorithm using the autodiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "using ForwardDiff\n",
    "\n",
    "function f(x) # f:R^4 -> R^4\n",
    "    f1 = x[1]+2*x[2]-x[1]*(x[1]+4*x[2])-x[2]*(4*x[1]+10*x[2])+3;\n",
    "    f2 = 3*x[1]+4*x[2]-x[1]*(x[1]+4*x[2])-x[2]*(4*x[1]+10*x[2])+4;\n",
    "    f3 = 0.5*cos(x[1])+x[3]-sin(x[3])^7;\n",
    "    f4 = -2*x[2]^2*sin(x[1])+x[4]^3;\n",
    "    return [f1; f2; f3; f4]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration: N = 7\n",
      "root: [-2.2595730873836666, 1.7595730873836664, 0.31808933104001225, -1.6845806986424827]\n",
      "f(root): [1.7763568394002505e-15, 0.0, -4.5428071027142636e-17, -3.4488767397533593e-10]\n"
     ]
    }
   ],
   "source": [
    "myfunc = f;\n",
    "x0 = [-2.; 3.; π; -1.]; # initial guess\n",
    "delta = 1e-9; # set a convergence threshold\n",
    "# set a max iteration so we don't get stuck in the loop forever!\n",
    "MAX_ITER = 100; \n",
    "N = 1; # counter for iteration\n",
    "x = x0; # root variable\n",
    "# let the fun begin!\n",
    "while N < MAX_ITER\n",
    "    fk = myfunc(x); # evaluate f at current guess, x0\n",
    "    A = ForwardDiff.jacobian(myfunc, x); # compute Jacobian using autodiff\n",
    "    if abs(det(A)) < delta\n",
    "        println(\"Newton-Raphson method did not converge; Jacobian is singular.\")\n",
    "        break\n",
    "    else\n",
    "        dx = -A \\ fk; # solve the linear system\n",
    "    end\n",
    "    if norm(dx) < delta\n",
    "        println(\"Converged at iteration: N = \", N)\n",
    "        break\n",
    "    else\n",
    "        x += dx;\n",
    "    end\n",
    "    N += 1;\n",
    "end\n",
    "\n",
    "print(\"root: \", x, \"\\n\")\n",
    "print(\"f(root): \", myfunc(x), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
